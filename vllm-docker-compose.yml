services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm_server
    ports:
      - "8000:8000"
    # Mapeia o diretório local em que o modelo safetensors está armazenado
    volumes:
      - ./volumes/checkpoints/hf_format/samples_44:/app/my_local_model
    env_file:
      - .env  # <-- Adicionando o arquivo .env aqui
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
    ipc: host  # Para compartilhamento de memória e melhor performance
    command: [
      "--host", "0.0.0.0",
      "--port", "8000",
      "--model", "file:///app/my_local_model",  # usando file:// ou apenas 'file://'
      "--dtype", "float16"
    ]
